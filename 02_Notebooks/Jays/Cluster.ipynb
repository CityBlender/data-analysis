{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alfie pymongo use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from pymongo import MongoClient\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ast\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "#need to translate from wgs to bng to do spatial clustering\n",
    "import pyproj\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#now to try out the clustering with this new data set\n",
    "#now import DBSCAN clustering package\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#import hierarchical clustering\n",
    "from scipy import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifies number of rows to show\n",
    "pd.set_option('display.max_rows', 300) \n",
    "\n",
    "# specifies default number format to 4 decimal places\n",
    "pd.options.display.float_format = '{:40,.4f}'.format \n",
    "\n",
    "# specifies that graphs should use ggplot styling\n",
    "plt.style.use('ggplot') \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 300) # specifies number of rows to show\n",
    "pd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .env variable\n",
    "# base_dir = 'D:\\Documentos\\GitHub\\CityBlender'\n",
    "# dotenv_file = os.path.join(base_dir, '.env')\n",
    "\n",
    "base_dir = 'C:/Users/Alfie/Documents/MSc Smart Cities/Term 2/Spatial Data Capture Storage and Analysis/Coursework/Analysis Test/'\n",
    "dotenv_file = os.path.join(base_dir, '.env')\n",
    "\n",
    "\n",
    "# Continue loading stuff\n",
    "\n",
    "if os.path.isfile(dotenv_file):\n",
    "    load_dotenv(dotenv_file, verbose=True)\n",
    "    \n",
    "db_uri = os.getenv('DB_URI')\n",
    "\n",
    "db_client = MongoClient(str(db_uri))\n",
    "\n",
    "\n",
    "# choose a database to connect to (.london)\n",
    "db_london = db_client.london\n",
    "status = db_client.london.command('serverStatus')\n",
    "\n",
    "\n",
    "# create the connections and build the local datasets\n",
    "db_london_events = db_london['events']\n",
    "db_london_artist = db_london['artists']\n",
    "\n",
    "allLondonEvents = db_london_events.find()\n",
    "\n",
    "londonEventsdf = pd.DataFrame(list(allLondonEvents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put this in the date format\n",
    "londonEventsdf['date'] = [(datetime.strptime(londonEventsdf['date'][x], '%Y-%m-%d')) for x in range(len(londonEventsdf['date']))]\n",
    "\n",
    "# Convert the dates into day name\n",
    "londonEventsdf['date_name'] = [(londonEventsdf['date'][x].strftime(\"%A\")) for x in range(len(londonEventsdf['date']))]\n",
    "\n",
    "# Lat & Long\n",
    "londonEventsdf['lat'] = [(londonEventsdf['location'][x]['lat']) for x in range(len(londonEventsdf['location']))]\n",
    "londonEventsdf['lng'] = [(londonEventsdf['location'][x]['lng']) for x in range(len(londonEventsdf['location']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dates in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get min and max date of this dataset\n",
    "print(min(londonEventsdf['date']))\n",
    "print(max((londonEventsdf['date'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some common projections using EPSG codes\n",
    "wgs84=pyproj.Proj(\"+init=EPSG:4326\") # LatLon with WGS84 datum used by GPS units and Google Earth\n",
    "osgb36=pyproj.Proj(\"+init=EPSG:27700\") # UK Ordnance Survey, 1936 datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a new pd without NaNs\n",
    "londonEventsdfNoNan = londonEventsdf[londonEventsdf['lat'].notnull()]\n",
    "\n",
    "#make new columns for the new latlng\n",
    "londonEventsdfNoNan['BNGnorthing'] = \"\"\n",
    "londonEventsdfNoNan['BNGeasting'] = \"\"\n",
    "\n",
    "\n",
    "londonEventsdfNoNan = londonEventsdfNoNan.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now run through the dataset and transform those coordinates\n",
    "origLat = []\n",
    "origLng = []\n",
    "\n",
    "for i in londonEventsdfNoNan['index']:\n",
    "    # print(i)\n",
    "    try:\n",
    "        origLat.append(londonEventsdfNoNan['lat'][i])\n",
    "        origLng.append(londonEventsdfNoNan['lng'][i])\n",
    "        \n",
    "    except:\n",
    "        origLat.append(np.nan)\n",
    "        origLng.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat,lon = pyproj.transform(wgs84,osgb36,origLng,origLat)\n",
    "    \n",
    "londonEventsdfNoNan['BNGeasting'] = lat\n",
    "londonEventsdfNoNan['BNGnorthing'] = lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "londonEventsdfNoNan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "londonEventsdfNoNan['BNGnorthing'] = pd.to_numeric(londonEventsdfNoNan['BNGnorthing'])\n",
    "londonEventsdfNoNan['BNGeasting'] = pd.to_numeric(londonEventsdfNoNan['BNGeasting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "londonEventsdfNoNan = londonEventsdfNoNan[londonEventsdfNoNan['BNGnorthing'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "londonEventsdfNoNan = londonEventsdfNoNan.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to subset the genere depending on which dataset\n",
    "def subset_by_genere(db = londonEventsdfNoNan, lib = 'spotify', words = 'reggae'):\n",
    "    return db[db[lib].astype(str).str.contains(words, case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reggaeEventsdf = subset_by_genere()\n",
    "\n",
    "reggaeEventsdf2 = subset_by_genere(lib = 'lastfm')\n",
    "allReggaeEvents = pd.concat([reggaeEventsdf,reggaeEventsdf2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of duplicates\n",
    "allReggaeEvents = allReggaeEvents.drop_duplicates(subset='_id', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try folk subset\n",
    "folkEvents = subset_by_genere(words = 'folk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folkEvents2 = folkEvents = subset_by_genere(lib = 'lastfm', words = 'folk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFolkEvents = pd.concat([folkEvents,folkEvents2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFolkEvents = allFolkEvents.drop_duplicates(subset='_id', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a genre compiling function\n",
    "def genre_df_array(genreName):\n",
    "    firstEventsdf = genreName+\"Eventsdf\" \n",
    "    secondEventsdf = genreName+\"Eventsdf2\"\n",
    "    firstEventsdf = subset_by_genere(lib='spotify',words = genreName)\n",
    "    secondEventsdf = subset_by_genere(lib='lastfm',words = genreName)\n",
    "    allEventsdf = \"all\"+genreName+\"Events\" \n",
    "    allEventsdf = pd.concat([firstEventsdf, secondEventsdf])\n",
    "    allEventsdf = allEventsdf.drop_duplicates(subset='_id', keep='last')\n",
    "    return allEventsdf\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_df_loc_array(genreDf):\n",
    "    genreLocsArray = genreDf[['BNGeasting', 'BNGnorthing']].values\n",
    "    return genreLocsArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allJazzEvents = genre_df(\"jazz\")\n",
    "jazzLocsArray = genre_df_loc_array(allJazzEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allJazzEvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazzLocsArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFolkEvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allReggaeEvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to put the lat and lng as an array\n",
    "locsAsArray = londonEventsdfNoNan[['BNGeasting', 'BNGnorthing']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locsAsArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now get reggae locs\n",
    "regLocsAsArray = allReggaeEvents[['BNGeasting', 'BNGnorthing']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regLocsAsArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folkLocsAsArray = allFolkEvents[['BNGeasting', 'BNGnorthing']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folkLocsAsArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ripley's k is rubbish, let's try using hierarchical clustering to work out how large to set the eps\n",
    "regCluster = cluster.hierarchy.linkage(regLocsAsArray)\n",
    "cluster.hierarchy.dendrogram(regCluster);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try out a dbscan of all reggae\n",
    "#create new dbscan cluster object\n",
    "dbscan500 = DBSCAN(eps=500, min_samples=10)\n",
    "dbscan750 = DBSCAN(eps=750, min_samples=10)\n",
    "dbscan1000 = DBSCAN(eps=1000, min_samples=10)\n",
    "dbscan1500 = DBSCAN(eps=1500, min_samples=10)\n",
    "dbscan2000 = DBSCAN(eps=2000, min_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get only locations from the reggae set\n",
    "#get just locations\n",
    "#use the locsArray again\n",
    "\n",
    "dbscanAllEvents = dbscan.fit(locsAsArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the labels of the groups\n",
    "dbscanLabelsAllEvents = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscanLabelsAllEvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now add the labels back into the reggae dataframe\n",
    "#need to reindex allReggaeEvents for it to work\n",
    "#londonEventsdfNoNan = londonEventsdfNoNan.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "londonEventsdfNoNan['dbscanLabelsAllEvents'] = pd.DataFrame(dbscanLabelsAllEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for creating a dbscan grouping of events\n",
    "def dbscan_genre_cluster(genredf, dbscan):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now give it a plot\n",
    "#define colorscheme\n",
    "\n",
    "londonEventsdfNoNan.plot.scatter(y='BNGnorthing', x='BNGeasting', c='dbscanLabelsAllEvents', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try clustering out on the whole dataset\n",
    "dbscan2ndRun = dbscan.fit(locsAsArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "londonEventsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
